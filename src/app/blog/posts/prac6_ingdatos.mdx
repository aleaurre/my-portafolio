---
title: "Integridad del Modelo: C√≥mo el Anti-Leakage y el Escalado Deciden el √âxito Predictivo en Ames Housing."
publishedAt: "2025-09-20"
tag: "Curso: Ingenier√≠a de Datos"
image: "/practica6/portada.jpg"
---

# Feature Scaling & Anti-Leakage (Ames Housing)

## Introducci√≥n

En el **an√°lisis de datos** aplicado espec√≠ficamente a problemas de **predicci√≥n inmobiliaria**, una situaci√≥n recurrente y desafiante es la de encontrar variables que exhiben **escalas marcadamente distintas** entre s√≠, presentan **distribuciones fuertemente sesgadas** (asimetr√≠a) y contienen **valores at√≠picos** o *outliers* que, de no tratarse, pueden **influir negativamente** en la solidez y el desempe√±o predictivo de los modelos de *Machine Learning*. Este escenario se manifest√≥ claramente en el *dataset* **Ames Housing**, el cual recopila una vasta y heterog√©nea colecci√≥n de informaci√≥n sobre propiedades residenciales. Estos datos abarcan desde variables de medici√≥n continua y discreta, como las **superficies** de los lotes y los pisos, hasta **puntuaciones de calidad** subjetivas asignadas por inspectores, lo que contribuye a la complejidad del preprocesamiento.

La presente pr√°ctica se articul√≥ en torno al estudio y la justificaci√≥n de la necesidad de aplicar el **escalado de caracter√≠sticas** (*Feature Scaling*). Se abord√≥ mediante la **comparaci√≥n met√≥dica** de distintos **transformadores** y **escaladores** estad√≠sticos, y se procedi√≥ a evaluar de forma emp√≠rica c√≥mo las decisiones tomadas en la fase de preprocesamiento de datos pod√≠an impactar significativamente en la precisi√≥n de la estimaci√≥n de la variable objetivo, que es el **valor de venta** (`SalePrice`). De igual importancia, se destin√≥ una secci√≥n a profundizar en el rol **fundamental** del concepto de **anti-leakage** (anti-filtraci√≥n de datos). Este principio se implement√≥ asegurando una **separaci√≥n estricta y rigurosa** entre los conjuntos de entrenamiento y validaci√≥n, lo cual se materializ√≥ mediante el uso estrat√©gico de **pipelines** de procesamiento y la t√©cnica de **validaci√≥n cruzada** (*Cross-Validation*). 

Este estudio de caso se posiciona como un ejemplo paradigm√°tico para comprender que la robustez y la solidez estad√≠stica de un modelo predictivo no dependen √∫nicamente de la sofisticaci√≥n o el rendimiento del algoritmo de *Machine Learning* elegido, sino, crucialmente, de la **integridad** y la **transparencia** del flujo de procesamiento de datos que antecede y acompa√±a al entrenamiento.

---

## Marco te√≥rico

### Importancia del escalado

La necesidad del escalado de caracter√≠sticas radica en el funcionamiento interno de **numerosos algoritmos de *Machine Learning***. Estos algoritmos basan sus decisiones, optimizaciones y c√°lculos en conceptos de **distancias euclidianas**, **magnitudes de variables** o la direcci√≥n de los **gradientes de error**. Si una caracter√≠stica particular exhibe valores intr√≠nsecamente muy grandes (por ejemplo, el √°rea en metros cuadrados, $10^3$) en comparaci√≥n con otra (por ejemplo, una puntuaci√≥n de calidad de 1 a 10), esta variable de mayor magnitud tiende a **dominar el proceso de optimizaci√≥n** del modelo. Tal dominio puede conducir a un fen√≥meno en el que el modelo termine **sobreajustando** caracter√≠sticas que son, en realidad, irrelevantes o menos importantes para el problema de predicci√≥n (Shalev-Shwartz & Ben-David, 2014). Por esta raz√≥n, el escalado es un paso **esencial** y a menudo **obligatorio** en las siguientes familias de modelos:

* **Regresi√≥n lineal con regularizaci√≥n** (**Ridge**, **Lasso**, *Elastic Net*), donde los t√©rminos de penalizaci√≥n dependen de las magnitudes de los coeficientes.
* **K-Nearest Neighbors (KNN)**, que depende directamente de la distancia entre puntos.
* **Support Vector Machines (SVM)**, cuyo margen de separaci√≥n se calcula en el espacio de caracter√≠sticas.
* **PCA** y otras t√©cnicas de **reducci√≥n de dimensionalidad** basadas en la varianza.
* **Modelos basados en redes neuronales** (*Deep Learning*), que se benefician de datos centrados y de varianza unitaria para la estabilidad del entrenamiento.

Es importante se√±alar que, si bien modelos no param√©tricos y basados en √°rboles de decisi√≥n, como **Random Forest** o **Gradient Boosting**, no requieren estrictamente el escalado para su funcionamiento l√≥gico (dado que operan con umbrales y no con distancias), el escalado **puede acelerar** marginalmente el entrenamiento.

---

### Transformaciones para corregir sesgos

Las distribuciones de variables con **colas largas** (un indicador de alta **skewness** o asimetr√≠a) son problem√°ticas porque, en el contexto de modelos de regresi√≥n lineal, violan el supuesto de **normalidad de los errores** y comprometen la **relaci√≥n lineal** que se espera establecer entre las caracter√≠sticas y la variable objetivo. Para mitigar esta asimetr√≠a, se emplean **transformaciones matem√°ticas** que "comprimen" la cola de la distribuci√≥n. A continuaci√≥n, se detallan tres de las transformaciones m√°s utilizadas:

<table style={{ width: '100%', borderCollapse: 'collapse' }}>
  <thead>
    <tr>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>M√©todo</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>F√≥rmula/Base</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Objetivo Principal</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Cu√°ndo usarlo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>`log1p`</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>$\log(1 + x)$</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Comprimir colas extremas ‚Üí menor influencia de valores extremos</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Variables **estrictamente no negativas** ($\ge 0$) con fuerte asimetr√≠a positiva.</td>
    </tr>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**Yeo-Johnson**</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Transformaci√≥n de potencia</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Normalizar la distribuci√≥n incluso ante la presencia de ceros o valores negativos</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Cuando log1p no es viable o suficiente; ofrece una transformaci√≥n m√°s flexible.</td>
    </tr>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**QuantileTransformer**</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Mapeo por percentiles</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Transformar la distribuci√≥n a una distribuci√≥n Gaussiana (Normal) o Uniforme</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Cuando se busca una **robustez extrema** ante *outliers*, a costa de alterar dr√°sticamente la estructura de los datos originales.</td>
    </tr>
  </tbody>
</table>

---

### Data leakage y pipelines

El **data leakage** (filtraci√≥n de datos) representa uno de los **errores metodol√≥gicos m√°s graves** en la ciencia de datos. Ocurre cuando se introduce, de manera **inadvertida**, informaci√≥n proveniente del conjunto de prueba (los datos que el modelo nunca deber√≠a haber visto) en la fase de entrenamiento. Un ejemplo cl√°sico y muy com√∫n es:

> **Error de *Leakage***: Calcular la media y la desviaci√≥n est√°ndar de la variable $X$ **sobre la totalidad del *dataset*** antes de realizar la separaci√≥n (*train-test split*). Posteriormente, se utiliza esta media global para alimentar el `StandardScaler`. Al hacerlo, se le est√° proporcionando al modelo informaci√≥n sobre la distribuci√≥n de los datos de prueba, lo que genera un **sesgo optimista**.

Esto genera **m√©tricas artificialmente optimistas** y puede llevar a decisiones err√≥neas en entornos reales (Kaufman et al., 2012). La **estrategia de defensa** adecuada y universalmente aceptada es doble:

‚úî **Aislamiento en *Pipelines***: Aplicar todas las transformaciones que requieren un c√°lculo (como `StandardScaler`, `MinMaxScaler`, o imputaci√≥n de valores faltantes) **estrictamente dentro de un objeto `Pipeline`** de scikit-learn. Este dise√±o asegura que el m√©todo `fit` del transformador se ejecute **solo con los datos de entrenamiento**.
‚úî **Evaluaci√≥n Honesta con *Cross-Validation***: Evaluar el rendimiento final del *pipeline* mediante **Validaci√≥n Cruzada** (*k-fold CV*). Esto garantiza que el modelo sea puesto a prueba de manera **transparente** contra m√∫ltiples subconjuntos de datos no vistos previamente. 

Este flujo de trabajo no solo combate el *leakage*, sino que tambi√©n asegura la **reproducibilidad** y la **transparencia metodol√≥gica** de todo el proceso.

---

## Metodolog√≠a

La pr√°ctica se focaliz√≥ en el preprocesamiento de un subconjunto espec√≠fico de **cinco variables num√©ricas** clave del *dataset* Ames Housing, elegidas por sus caracter√≠sticas de sesgo y escala:

* `Lot Area` (√Årea del Lote)
* `Overall Qual` (Calidad General)
* `Year Built` (A√±o de Construcci√≥n)
* `1st Flr SF` (Superficie del Primer Piso)
* `Gr Liv Area` (Superficie Habitable en Planta Baja)

Y la **variable objetivo** a predecir: **`SalePrice`** (Precio de Venta).

El flujo de trabajo metodol√≥gico fue riguroso y secuencial:

1.  **Diagn√≥stico Inicial**: Se realiz√≥ un an√°lisis exploratorio para determinar las **escalas originales**, el grado de **sesgo** (asimetr√≠a) y la presencia de **valores at√≠picos** en cada variable.
2.  **Prueba de Transformaciones**: Se experiment√≥ con diversas **transformaciones avanzadas** (`log1p`, Yeo-Johnson, etc.) para corregir el sesgo y acercar las distribuciones a la normalidad.
3.  **Comparaci√≥n de Escaladores**: Se evaluaron las diferencias entre distintos **escaladores** (como `StandardScaler`, `MinMaxScaler`, `MaxAbsScaler`).
4.  **Evaluaci√≥n de Leakage**: Este fue un punto central, comparando expl√≠citamente tres modelos (M1, M2, M3) con **distintos grados de *leakage*** para cuantificar su impacto en las m√©tricas.
5.  **Validaci√≥n Estad√≠stica Final**: Se emple√≥ una validaci√≥n cruzada de **5 pliegues (5-fold CV)** para obtener una m√©trica de error promedio m√°s **robusta y confiable**.

El **modelo base** utilizado para todas las comparaciones fue la **Regresi√≥n Lineal**. Este modelo fue seleccionado precisamente por su **alta interpretabilidad** y por su **extrema sensibilidad** a las decisiones tomadas durante el preprocesamiento de los datos, lo que lo convierte en un excelente *benchmark*.

---

## Resultados

### Diferencias en escalas

`Lot Area` present√≥ una escala mucho mayor que el resto, confirmando la necesidad de normalizar o estandarizar antes del modelado.

<div style={{ display: "flex", justifyContent: "center", margin: "24px 0" }}>
¬† <Image
¬† ¬† src="/practica6/boxplots.png"
¬† ¬† alt="Boxplots mostrando diferencias de escala"
¬† ¬† width={800}
¬† ¬† height={600}
¬† />
</div>

El boxplot de las variables num√©ricas revel√≥ una disparidad extrema en sus rangos. Mientras que caracter√≠sticas como Overall Qual (calidad) se mov√≠an en un rango peque√±o de 1 a 10, y Year Built (a√±o) se manten√≠a en un rango de tres d√≠gitos, la variable Lot Area (√°rea del lote) se extend√≠a a trav√©s de cinco o seis d√≠gitos, superando las 200,000 unidades. Esta gran diferencia en magnitud es la raz√≥n fundamental por la cual los modelos basados en distancia o gradiente (como la Regresi√≥n Lineal o KNN) fallar√≠an sin un preprocesamiento adecuado, ya que la variaci√≥n en Lot Area dominar√≠a el c√°lculo del error. La visualizaci√≥n confirm√≥ que el primer paso cr√≠tico era la homogeneizaci√≥n de las magnitudes de todas las features.

---

### Sesgo y outliers

El **skew** inicial de `Lot Area` fue de 12.8 ‚Üí indicativo de cola extremadamente larga.

Tras aplicar `log1p`:
**skew = ‚Äì0.49** ‚Üí distribuci√≥n casi sim√©trica

<div style={{ display: "flex", justifyContent: "center", margin: "24px 0" }}>
  <div style={{ position: "relative", width: "100%", height: "300px" }}>
¬†   <Image
¬† ¬†   src="/practica6/comparacionlogs.png"
¬† ¬†   alt="Evoluci√≥n de la distribuci√≥n tras log1p"
      fill
¬†   />
  </div>
</div>

La imagen con los tres histogramas ilustra de manera contundente la efectividad del proceso de transformaci√≥n. El gr√°fico de la izquierda (rojo), "Original: Lot Area", muestra una distribuci√≥n fuertemente concentrada cerca de cero, con una cola larga y delgada que se extiende hacia la derecha (alta asimetr√≠a positiva), evidenciando la presencia de outliers de gran magnitud. El gr√°fico central (naranja), "Log Transform: Lot Area", muestra el resultado inmediato de aplicar la funci√≥n $\log(1 + x)$ (log1p). La distribuci√≥n ha sido comprimida y simetrizada, eliminando la cola larga y concentrando los datos en una forma que se asemeja mucho m√°s a una distribuci√≥n normal. Finalmente, el gr√°fico de la derecha (verde), "Log + Scaled", muestra la distribuci√≥n despu√©s de aplicar un StandardScaler al resultado logar√≠tmico. Esta estandarizaci√≥n centra la distribuci√≥n alrededor de la media (aproximadamente cero) y ajusta la varianza a la unidad, lo que la deja perfectamente lista para ser consumida por un modelo basado en gradiente, confirmando que la combinaci√≥n de $\log(1 + x)$ y StandardScaler es la secuencia de preprocesamiento √≥ptima para esta variable.

---

La identificaci√≥n de outliers con IQR y Z-score vari√≥ seg√∫n el *split* de datos, lo cual motiv√≥ incorporar **validaci√≥n cruzada sistem√°tica**.

<div style={{ display: "flex", justifyContent: "center", margin: "24px 0" }}>
¬† <Image
¬† ¬† src="/practica6/histogramas.png"
¬† ¬† alt="Comparativa visual de outliers"
¬† ¬† width={800}
¬† ¬† height={600}
¬† />
</div>

Aunque la transformaci√≥n logar√≠tmica es una forma eficaz de mitigar la influencia de los outliers de gran magnitud al "encoger" sus valores, el an√°lisis exploratorio confirm√≥ que el conjunto de datos a√∫n conten√≠a valores extremos que podr√≠an ser considerados an√≥malos. La decisi√≥n de no eliminar estos outliers de forma agresiva antes del split de entrenamiento-prueba es crucial, ya que esto podr√≠a inducir leakage. El proceso de validaci√≥n cruzada se adopt√≥ como el m√©todo m√°s honesto para tratar la variabilidad de estos puntos. Al rotar los pliegues, garantizamos que el modelo sea evaluado contra diferentes subconjuntos de outliers, obteniendo un error promedio que refleja el rendimiento real y robusto, en lugar de un error optimista basado en la eliminaci√≥n sesgada de puntos de datos.

---

### Transformadores evaluados

Se analizaron cinco transformadores presentes en scikit-learn:

<div style={{ display: "flex", flexDirection: "column", gap: "24px", margin: "24px 0" }}>
  <div style={{ position: "relative", width: "100%", height: "300px" }}>
    <Image src="/practica6/FunctionTransformer.png" alt="FunctionTransformer log1p" fill />
  </div>
  <div style={{ position: "relative", width: "100%", height: "300px" }}>
    <Image src="/practica6/YeoJohnson.png" alt="Yeo-Johnson" fill />
  </div>
  <div style={{ position: "relative", width: "100%", height: "300px" }}>
    <Image src="/practica6/QuantileTransformer.png" alt="QuantileTransformer" fill />
  </div>
  <div style={{ position: "relative", width: "100%", height: "300px" }}>
    <Image src="/practica6/MaxAbsScaler.png" alt="MaxAbsScaler" fill />
  </div>
  <div style={{ position: "relative", width: "100%", height: "300px" }}>
    <Image src="/practica6/NormalizerL2.png" alt="Normalizer L2" fill />
  </div>
</div>

`QuantileTransformer` alcanz√≥ la mejor normalizaci√≥n, pero a costa de perder significado en las distancias originales.

**`log1p`** fue el balance m√°s adecuado entre interpretabilidad y desempe√±o.

---

### Evaluaci√≥n del leakage

Se compararon tres escenarios de modelado, demostrando el impacto del rigor metodol√≥gico:

<table style={{ width: '100%', borderCollapse: 'collapse' }}>
  <thead>
    <tr>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Modelo</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Pipeline</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>CV</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Leakage</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**M1**</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>No</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>No</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>S√≠</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>32185</td>
    </tr>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**M2**</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Parcial</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>No</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Reducido</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>32287</td>
    </tr>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**M3**</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>S√≠</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>5-fold</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>No</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**30651**</td>
    </tr>
  </tbody>
</table>

**Conclusi√≥n**:
El pipeline **no mejora el modelo** intr√≠nsecamente, sino la **honestidad de la evaluaci√≥n** de su rendimiento.

---

### M√©tricas finales consolidadas

Las m√©tricas finales obtenidas con el modelo M3 (con *pipeline* y 5-fold CV) fueron:

<table style={{ width: '100%', borderCollapse: 'collapse' }}>
  <thead>
    <tr>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Indicador</th>
      <th style={{ border: '1px solid #ddd', padding: '8px' }}>Valor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>R¬≤ (5-fold CV)</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>**0.776 ¬± 0.033**</td>
    </tr>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>RMSE promedio</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>30651</td>
    </tr>
    <tr>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>Mejor transformador</td>
      <td style={{ border: '1px solid #ddd', padding: '8px' }}>log1p + StandardScaler</td>
    </tr>
  </tbody>
</table>

---

## Discusi√≥n

Los resultados muestran que el escalado es un paso fundamental que:

* **Acelera la convergencia** y reduce los errores en modelos lineales.
* Normaliza la importancia relativa entre las variables de diferente magnitud.
* Posibilita una **mejor modelizaci√≥n de patrones** al mitigar la influencia desproporcionada de valores extremos.
* Sin embargo, se debe aplicar con criterio: algunas variables **ya est√°n normalizadas por dise√±o** (e.g. `Overall Qual`) y no requieren escalado.

En cuanto al *leakage*, la evaluaci√≥n inicial con *single split* (M1) parec√≠a indicar un modelo razonable. Sin embargo, al aplicar la **Validaci√≥n Cruzada con *Pipeline*** (M3), las m√©tricas se hicieron m√°s conservadoras, pero reflejan un error m√°s consistente:

> Lo que se pierde en ‚Äúoptimismo‚Äù en las m√©tricas, se gana en **confiabilidad cient√≠fica** en el entorno real.

---

## Conclusiones

El **pipeline recomendado final** para garantizar la solidez metodol√≥gica y el mejor rendimiento fue:

* Transformaci√≥n **log1p** en variables con fuerte asimetr√≠a positiva.
* Imputaci√≥n de valores faltantes y One-Hot Encoding dentro del *pipeline*.
* **`StandardScaler`** como escalador por defecto para estandarizar las caracter√≠sticas.
* Regresi√≥n lineal como *baseline* interpretable y reproducible.

El principio de **anti-leakage** demostr√≥ ser **imprescindible** y no una mera recomendaci√≥n te√≥rica. Se concluye que:

> Preprocesar correctamente los datos y asegurar la integridad del flujo de trabajo vale m√°s que elegir el algoritmo ‚Äúm√°s avanzado‚Äù.

---

## Referencias

Box, G. E. P., & Cox, D. R. (1964). *An analysis of transformations*. Journal of the Royal Statistical Society: Series B, 26(2), 211-252.

Kaufman, A., Rosset, S., Perlich, C., & Stitelman, O. (2012). Leakage in data mining: Formulation, detection, and avoidance. *ACM SIGKDD*, 556-563.

Shalev-Shwartz, S., & Ben-David, S. (2014). *Understanding Machine Learning: From Theory to Algorithms*. Cambridge University Press.

---
## Evidencias

Para consultar el c√≥digo √≠ntegro, las funciones aplicadas, la generaci√≥n de gr√°ficos y los resultados intermedios de la pr√°ctica, se adjunta el notebook original:

üìé **Google Colab:** [Abrir notebook](https://colab.research.google.com/drive/1Wo1mBYS5jdOH1bBc1d3N2-BggcUT3zgQ?usp=sharing)

---
